# Tech Mahindra Interview Answers - AI Voice Tutoring Platform

## ðŸ”· **1. High-Level Architecture & Flow**

### Q1: Walk me through your system architecture end-to-end. How does a voice call start, flow through Vapi, Deepgram, GPT-4, and finally respond via ElevenLabs?

**Answer:**
Sure! So when a user clicks "Start Session" on a companion page, we initialize the Vapi SDK from the frontend. The flow works like this:

First, we configure an assistant using our `configureAssistant` utility which sets up three key providers - Deepgram Nova-3 for transcription, GPT-4 as the language model, and ElevenLabs for voice synthesis. When `vapi.start()` is called, it establishes a WebSocket connection.

Once the call starts, Deepgram begins streaming real-time speech-to-text. As the user speaks, partial and final transcripts are sent through Vapi to GPT-4, which has been primed with a system prompt containing the companion's subject, topic, and teaching style as template variables. GPT-4 generates the tutoring response based on this context, and that text is immediately sent to ElevenLabs, which converts it to speech using the configured voice ID.

The audio streams back through Vapi to the user's browser. Throughout this, we're listening to Vapi events on the client side - like `call-start`, `message`, `speech-start`, and `speech-end` - to update the UI and maintain the transcript history. The entire pipeline is orchestrated by Vapi, so we don't manage the connections between these services directly.

---

### Q2: How does your backend store and manage companion configurations?

**Answer:**
We use Supabase as our database. Each companion is stored with fields like name, subject, topic, duration, voice type, teaching style, and the author's user ID from Clerk.

When a user creates a companion through our form, the `createCompanion` server action validates that no duplicate exists for that user with the same name and topic to avoid redundancy. The companion configuration includes the voice selection - like "Sarah" or "Stella" - and style like "casual" or "professional," which we map to specific ElevenLabs voice IDs in our constants file.

These configurations are retrieved server-side when the user navigates to a companion session page. We pass the subject, topic, style, and voice to the frontend component, which then uses them as `variableValues` in the Vapi assistant override. This way, the GPT-4 system prompt gets dynamically populated with the right context for that specific tutoring session.

---

### Q3: How do you maintain session state during a voice call? Where is context stored?

**Answer:**
The conversation context is primarily maintained by Vapi and GPT-4's conversation history. On our end, we use React state to track the call status - whether it's inactive, connecting, active, or finished.

We also maintain a local `messages` array that stores final transcript messages with their roles - user or assistant. This is built by listening to Vapi's message events and filtering for transcript type "final" messages. This gives us a client-side conversation history that we display in the UI.

For persistent storage, when a call ends, we trigger the `addToSessionHistory` function which creates a record in our `session_history` table linking the user ID, companion ID, and timestamp. We don't store full transcripts in the database currently, but the session history allows us to track which companions a user has interacted with and show them in their recent sessions on the dashboard.

---

### Q4: What happens when the user interrupts the AI mid-sentence? Explain Vapi's interrupt model and how your logic handles it.

**Answer:**
Vapi has built-in interrupt handling. When it detects the user starting to speak while the assistant is still talking, it automatically stops the ElevenLabs audio playback and signals an interrupt.

On our side, we handle this through the `speech-start` and `speech-end` events. When `speech-end` fires, we set `isSpeaking` to false, which stops our Lottie soundwave animation. The key thing is that Vapi manages the actual audio cutoff and context switching, so the user's new input immediately goes to Deepgram for transcription.

The GPT-4 conversation context is preserved because Vapi maintains the message history. So when the user interrupts, their new transcript is appended to the conversation, and GPT-4 can respond appropriately, understanding that the user interjected. We don't need to manually handle buffer flushing or state recovery - Vapi's infrastructure takes care of that complexity.

---

## ðŸ”· **2. Voice Pipeline & Real-Time Constraints**

### Q5: How does Deepgram streaming ASR connect with GPT-4 and ElevenLabs? Are they sequential or parallel?

**Answer:**
They're orchestrated by Vapi in a pipelined manner. The flow is essentially sequential but optimized for streaming. Deepgram continuously transcribes speech in real-time, sending both partial and final transcripts.

When a final transcript is ready - meaning the user has finished a complete thought or paused - that text is sent to GPT-4. GPT-4 then generates a response, and as soon as the first tokens start streaming out, they're forwarded to ElevenLabs for speech synthesis.

ElevenLabs can also stream audio, so the user might start hearing the response before GPT-4 has even finished generating the complete text. This streaming pipeline is what keeps latency low. Vapi handles all the buffering and coordination, so from our application's perspective, we just configure these providers and let Vapi manage the data flow between them.

---

### Q6: How do you ensure low latency in the voice response? What were your bottlenecks?

**Answer:**
We optimized for latency in several ways. First, we use Deepgram's Nova-3 model which is specifically designed for low-latency streaming. For GPT-4, we keep system prompts concise - under 250 words - to reduce processing overhead while still maintaining context.

On the ElevenLabs side, we configured voice settings with speed at 0.9 and optimized stability and similarity boost parameters. We also enabled speaker boost for clarity. The voice synthesis happens in chunks as GPT-4 streams tokens, which reduces time-to-first-audio.

The main bottleneck we observed was during peak times or with complex queries where GPT-4 takes longer to generate responses. To mitigate this, we instruct the model in the system prompt to keep responses under 150 words and conversational, which keeps generation time reasonable. Network latency between the user and Vapi's servers is another factor, but that's largely out of our control.

---

### Q7: What happens if transcription is delayed but TTS is ready?

**Answer:**
In practice, this scenario doesn't really occur because the pipeline is sequential. TTS can't be "ready" before transcription completes because ElevenLabs needs the text from GPT-4, which in turn needs the transcript from Deepgram.

However, if Deepgram has a delay in finalizing a transcript, Vapi will wait for that final transcript before triggering GPT-4. During this time, the user would see the call as active but the assistant wouldn't respond yet. We show a speaking indicator using the Lottie animation, so if the assistant isn't speaking, the animation stops, giving visual feedback that the system is processing.

If the delay is significant, Vapi would eventually timeout or the user might speak again, which would cancel the previous incomplete input. Our UI remains responsive throughout because we're updating based on event listeners rather than blocking operations.

---

### Q8: How do you handle packet loss or network glitches during a call?

**Answer:**
Vapi's WebSocket connection has built-in reconnection logic. If there's temporary packet loss, the connection will attempt to reestablish automatically. On our end, we listen for error events using `vapi.on('error')` and log them for monitoring.

In case of significant network disruption where the call drops completely, the `call-end` event would fire, and we'd update the UI to show that the session finished. The user would need to restart the session manually.

We don't have explicit retry logic on our side because Vapi handles the connection resilience. However, one improvement we could make is showing a "reconnecting" state in the UI based on network status or error events, rather than just ending the session. Currently, we rely on Vapi's infrastructure for handling transient network issues.

---

## ðŸ”· **3. Next.js Implementation**

### Q9: Which components in your project are Server Components and which are Client Components? Why?

**Answer:**
All our page components are Server Components by default in Next.js 15. This includes `CompanionSessionPage`, `CompanionPage`, and the main landing page. These components fetch data server-side using server actions like `getCompanion` and `getAllCompanions`, which directly query Supabase.

Client Components are marked with `'use client'` and include `CompanionComponent` - which manages the Vapi SDK and call state, `SearchInput`, `SubjectFilter`, and `CompanionForm`. These need to be client-side because they use hooks like `useState`, `useEffect`, and handle user interactions like form submissions and real-time call events.

The pattern we follow is: server components for data fetching and initial page rendering, client components for interactive features and state management. This gives us the benefits of SSR for performance and SEO while keeping interactive features fully dynamic. For example, `CompanionSessionPage` fetches the companion data server-side and passes it as props to `CompanionComponent` on the client.

---

### Q10: How do you fetch recent session data on the home page? Why Next.js fetch is suitable here?

**Answer:**
We use the `recentSession` server action which queries Supabase's `session_history` table. It joins with the companions table to get full companion details, orders by `created_at` in descending order, and limits the results - default is 10.

This happens server-side in the page component, so the data is fetched during SSR. Next.js fetch is suitable because it's automatically cached and can be revalidated. For a dashboard or home page showing recent sessions, we want this data to be reasonably fresh but don't need real-time updates.

Next.js also allows us to use revalidation strategies. If we wanted, we could add `revalidate: 60` to refetch this data every minute without user interaction. The server-side fetch means the initial page load is fast with pre-rendered HTML, improving perceived performance compared to client-side fetching where users would see loading states.

---

### Q11: Did you use ISR, SSR, or SSG for the landing pages of companions? Why?

**Answer:**
We're using SSR with dynamic rendering for the companions listing page. The page accepts search parameters for filtering by subject and topic, which means the content is user-specific and can't be statically generated at build time.

Each time a user visits `/companions` with different filters, Next.js renders that page server-side with the filtered companion data from Supabase. This ensures users always see the most up-to-date companions, including newly created ones.

For the individual companion session page at `/companions/[id]`, we're also using SSR because we need to fetch the specific companion data and validate user authentication in real-time. These pages are protected routes requiring Clerk authentication, so static generation wouldn't work.

If we wanted to optimize further, we could implement ISR for the main companions listing without filters and revalidate it periodically, but given that our companion library can be updated frequently by users, on-demand SSR gives us the flexibility we need.

---

### Q12: How do you pass user tier data into server components?

**Answer:**
We use Clerk's authorization system with the `auth()` function in server components. Clerk allows us to define features and plans in their dashboard, and we can check these using the `has()` method.

For example, in our `newCompanionPermission` server action, we call `const { userId, has } = await auth()` and then check `has({ plan: 'pro_learner' })` or `has({ feature: '3_active_companions' })`. Based on these checks, we enforce limits on how many companions a user can create.

This data isn't explicitly "passed" as props but rather queried server-side wherever we need authorization checks. The Clerk session is available in all server components and API routes through the authentication middleware. This approach keeps the authorization logic centralized and secure on the backend rather than relying on client-side tier information which could be manipulated.

---

## ðŸ”· **4. State Management & UI**

### Q13: How do you manage live call status (mute, end call, transcript updates) on the frontend?

**Answer:**
We use React's `useState` for managing call-related state in the `CompanionComponent`. We have state variables for `callStatus` (inactive, connecting, active, finished), `isSpeaking` (boolean for the AI speaking indicator), `isMuted` (microphone state), and `messages` (array of transcript messages).

These states are updated through Vapi event listeners that we set up in a `useEffect` hook. For example, when the `call-start` event fires, we set `callStatus` to ACTIVE. When `speech-start` fires, we set `isSpeaking` to true, which triggers our Lottie animation.

For user actions like mute and end call, we have handler functions: `toggleMic()` calls `vapi.setMuted()` and updates local state, while `handleDisconnect()` calls `vapi.stop()` and sets status to FINISHED. The UI re-renders based on these state changes, showing or hiding controls, animations, and transcript messages. It's a straightforward event-driven state management pattern.

---

### Q14: How do you ensure UI stays in sync with Vapi events?

**Answer:**
We establish Vapi event listeners in a `useEffect` hook that runs once on component mount. The listeners are registered for all critical events: `call-start`, `call-end`, `message`, `speech-start`, `speech-end`, and `error`.

Each listener updates the corresponding React state, which triggers re-renders. For example, when a new transcript message arrives, the `message` listener checks if it's a final transcript, then uses `setMessages()` to add it to the state array.

Importantly, we return a cleanup function from the `useEffect` that removes all event listeners using `vapi.off()`. This prevents memory leaks and duplicate listeners if the component re-mounts.

The synchronization is reliable because Vapi's event system is the single source of truth. We don't try to predict or anticipate events - we purely react to them. This keeps our UI perfectly aligned with the actual call state managed by Vapi's infrastructure.

---

## ðŸ”· **5. Authentication & Authorization (Clerk)**

### Q15: How do you guard routes such as `/dashboard` or `/call/[id]` using Clerk middleware?

**Answer:**
We use Clerk's `clerkMiddleware()` in our `middleware.ts` file, which automatically protects all routes. The middleware runs on every request based on the matcher configuration, which excludes static files and Next.js internals but includes API routes and dynamic pages.

For specific page-level protection, like the companion session page, we additionally check authentication server-side using `currentUser()` from Clerk. If the user is not authenticated, we redirect them to `/sign-in` using Next.js's `redirect()` function.

This dual-layer approach ensures that even if someone bypasses the middleware somehow, the page component itself will check authentication before rendering. For API routes, we use `auth()` to get the userId and validate it exists before performing any database operations. This prevents unauthorized API access even if someone crafts manual requests.

---

### Q16: Explain how you implemented tier-based feature gating.

**Answer:**
We implemented feature gating using Clerk's authorization features combined with server-side enforcement. In Clerk's dashboard, we've defined plans and features like '3_active_companions', '10_active_companions', and 'pro_learner' plan.

In our `newCompanionPermission` server action, we check the user's tier using the `has()` method. If they have the 'pro_learner' plan, we allow unlimited companions. Otherwise, we check for specific feature flags and set a limit accordingly - 3 or 10 companions.

We then query Supabase to count how many companions the user has already created. If they've reached their limit, we return false, which prevents the form submission on the frontend. This check happens server-side, so it's secure.

On the frontend, we call this permission check before showing the create companion form or enabling the submit button. This gives users immediate feedback about their tier limits without allowing them to waste time filling out forms they can't submit.

---

### Q17: How do you restrict number of companions per plan? Where is this enforcedâ€”frontend, backend, or both?

**Answer:**
It's enforced on both layers for good UX and security. On the frontend, we could call the `newCompanionPermission` function and disable the create button or show upgrade prompts if the user has reached their limit.

On the backend, the enforcement happens in the `createCompanion` server action. Before inserting a new companion into Supabase, we verify the user's tier and companion count. If they've exceeded their plan's limit, we throw an error or return a failure response.

This dual enforcement is critical. Frontend checks provide immediate user feedback and prevent unnecessary form submissions. Backend checks ensure security because someone could bypass the UI by sending direct API requests using tools like Postman or by manipulating the browser console. The server-side check is the actual security boundary that cannot be circumvented.

---

### Q18: What happens if a user tries to bypass UI controls using API calls?

**Answer:**
All our data mutations go through server actions which run on the backend. Each server action starts with authentication verification using `await auth()` from Clerk. If the request doesn't have a valid session token, Clerk will reject it before our code even executes.

For operations like creating companions, even if someone manages to authenticate but tries to exceed their tier limits via direct API calls, our `createCompanion` action includes the tier validation logic. We check their plan using `has()` and count their existing companions. If they're over the limit, the operation fails with an error.

The key is that all business logic and authorization checks live server-side. The frontend UI is just a convenience layer. Even if someone completely bypasses it, they can't do anything the backend doesn't explicitly allow based on their authenticated session and authorization tier.

---

## ðŸ”· **6. Billing, Subscription & Webhooks**

### Q19: How are subscription events handled?

**Answer:**
Currently, subscription management is handled through Clerk's built-in features and plans. When a user subscribes or changes their plan, Clerk updates their session claims automatically. These updated claims are immediately available the next time we call `auth()` or `currentUser()` in our server components or actions.

We check subscription status using the `has()` method which reads from these claims. So if a user upgrades to 'pro_learner', the next time they try to create a companion, our `newCompanionPermission` check will see their new plan and allow unlimited companions.

If we wanted more control, we could implement Clerk webhooks to listen for subscription events and update our own database with subscription details. This would allow us to track subscription history, send custom emails, or trigger other business logic. But for our current implementation, Clerk's automatic session claim updates are sufficient.

---

### Q20: If a user downgrades, how do you deal with extra companions they created earlier?

**Answer:**
That's a great question. Currently, we don't have explicit logic to handle downgrades. If a user with 10 companions downgrades to a plan that allows only 3, they would keep all 10 existing companions because we only check the limit during creation, not during reads.

The enforcement happens when they try to create new companions - they'd be blocked until they delete enough to get back under their new limit. This is a common SaaS pattern - you don't delete user data on downgrade, you just restrict new creation.

However, a better implementation would be to add a "soft delete" or "archive" feature. On downgrade, we could mark excess companions as inactive or archived, preventing them from being used but not permanently deleting them. If the user upgrades again, these companions could be restored.

We'd implement this through a webhook from Clerk that triggers when a subscription changes, then we'd run logic to compare the new limit against existing companions and handle accordingly.

---

### Q21: How do you verify webhook authenticity from Clerk?

**Answer:**
Clerk provides webhook signatures that you verify using their SDK. When setting up a webhook endpoint, Clerk gives you a signing secret. In your API route that receives webhooks, you'd use Clerk's `Webhook` utility to verify the signature.

The typical pattern is to get the raw request body and the signature from the `svix-signature` header, then call a verification method that Clerk provides. If verification fails, you reject the webhook with a 401 or 403 response.

This ensures that the webhook actually came from Clerk and wasn't spoofed by a malicious actor trying to trigger unauthorized subscription changes or user updates in your system. Without this verification, anyone could send fake webhook payloads to your endpoint and potentially manipulate user data.

We haven't implemented custom webhooks yet in this project, but if we did, webhook signature verification would be the first thing we'd add to the handler for security.

---

## ðŸ”· **7. Backend APIs & Security**

### Q22: Explain your API route structure in Next.js. How do you validate requests?

**Answer:**
We primarily use Next.js server actions rather than traditional API routes for data mutations. Server actions are defined with `'use server'` and are automatically exposed as secure endpoints that can only be called from your application.

For the API routes we do have, like in `app/api`, they follow Next.js 15's route handler pattern. Each route has a `route.ts` file that exports functions like `GET`, `POST`, etc.

Request validation happens at multiple levels. First, Clerk middleware ensures authentication. Second, in server actions, we use `await auth()` to get the userId and verify they're logged in. Third, for form data, we use Zod schemas with react-hook-form to validate input shape and types before submission.

Inside the server action, we perform business logic validation - like checking companion limits or verifying the user owns the resource they're trying to modify. Any database errors are caught and returned as error messages rather than exposing internal details.

---

### Q23: How do you stop unauthorized calls to create companions?

**Answer:**
Every server action starts with `const { userId } = await auth()` from Clerk. If there's no valid session, this will be undefined, and we can short-circuit the operation.

Additionally, when creating a companion, we automatically set the `author` field to the authenticated userId from the session, not from client input. This means a user can only create companions under their own account.

Even if someone tried to modify the request to set a different author ID, we override it with `author: userId` from the server-side session. This ensures that authentication is always tied to the session token, which can't be forged because Clerk's JWT is cryptographically signed.

The combination of Clerk middleware, server-side auth checks, and automatically setting ownership fields means there's no way to create companions without proper authentication or create them under someone else's account.

---

### Q24: How do you rate-limit companion creation or call initiation?

**Answer:**
Currently, we don't have explicit rate limiting implemented with a library like `upstash/ratelimit` or `express-rate-limit`. Our rate limiting is implicit through the tier-based companion limits.

For call initiation, Vapi itself has rate limits on their platform, and we're using their SDK which respects those limits. On our end, once a call is active, the UI disables the start button, preventing users from starting multiple simultaneous calls.

If we needed more robust rate limiting, we'd implement it using a Redis-based solution like Upstash. We'd key it by userId and track requests per time window - for example, allowing only 5 companion creations per hour, or 20 call initiations per day.

This would be implemented in the server actions before the main logic, checking the rate limit and returning early if exceeded. It's definitely something we'd add for production scale to prevent abuse and control costs from the third-party APIs.

---

### Q25: How do you sanitize user input before injecting into GPT-4 system prompts?

**Answer:**
This is critical for preventing prompt injection attacks. In our `configureAssistant` function, the user-provided values like subject, topic, and style are passed as template variables using Vapi's `variableValues` feature.

These variables are injected into predefined template slots like `{{ topic }}` and `{{ subject }}` in our system prompt. Vapi handles the template substitution, which provides a layer of safety because the overall prompt structure is fixed server-side.

However, we should add explicit input sanitization in the `createCompanion` server action. We'd strip or escape special characters that could break out of the template context, limit input length to reasonable values - like 100 characters for topic - and validate against allowed values for fields like subject.

Using Zod schemas for validation would help enforce these constraints before data even reaches the database. We'd also avoid directly concatenating user input into prompts and stick with the parameterized template variable approach, which is much safer.

---

## ðŸ”· **8. Audio + AI Logic**

### Q26: Explain your prompt engineering for controlling a companion's personality.

**Answer:**
Our system prompt is designed to be dynamic yet structured. The core structure defines the AI as an expert tutor conducting a live voice session, with clear goals to stay focused on the specified topic and build understanding progressively.

We inject three key variables: `{{ topic }}`, `{{ subject }}`, and `{{ style }}`. The style variable is particularly important for personality - it might be "casual mentor" or "professional instructor", and we explicitly tell GPT-4 to channel that tone throughout the conversation.

The prompt includes specific guidelines for voice sessions: keep responses under 150 words, use conversational language appropriate for speech, check in with questions frequently, chunk material into small segments, and avoid formatting that doesn't work in audio like emojis or asterisks.

By constraining response length and emphasizing conversational flow, we create tutoring companions that feel natural and engaging rather than robotic or long-winded. The teaching style parameter allows users to customize personality while maintaining the educational focus through the fixed prompt structure.

---

### Q27: How do you store and use call transcripts?

**Answer:**
Currently, we store transcripts temporarily in client-side React state during the active session. The `messages` array captures final transcript messages with their roles - user or assistant - by listening to Vapi's message events.

These transcripts are displayed in real-time in the UI so users can review what was said during the session. However, once the session ends and the component unmounts, these transcripts are lost. We don't persist them to the database yet.

For production use, we'd want to store transcripts in Supabase. We could create a `transcripts` table with columns for session_id, timestamp, role, and content. When the call ends, we'd loop through the messages array and bulk insert them.

This would enable features like reviewing past sessions, analyzing learning progress, or even training custom models on user interactions. We'd need to consider privacy implications and get user consent for storing conversations, especially in an educational context.

---

### Q28: How do you avoid hallucinations during tutoring calls?

**Answer:**
We use several strategies in our prompt engineering. First, we explicitly instruct the model to "stay laser-focused on {{ topic }} within {{ subject }}â€”no tangents." This keeps responses grounded in the specific educational content.

Second, we structure the tutoring approach to break information into small chunks and frequently check understanding with questions. This forces the model to be more Socratic and interactive rather than generating long, potentially inaccurate lectures.

Third, by using GPT-4 specifically rather than a smaller model, we get more reliable and factually accurate responses, especially for educational content. GPT-4 has better reasoning and is less prone to confident fabrications.

That said, we don't have explicit fact-checking or RAG integration yet. For more advanced applications, we'd want to implement retrieval-augmented generation where the model pulls from a vetted knowledge base specific to each subject. This would ensure responses are grounded in verified educational content rather than the model's training data alone.

---

### Q29: How do you provide subject-specific responses? Did you use RAG or system-prompt knowledge bases?

**Answer:**
Currently, we rely purely on GPT-4's pre-trained knowledge combined with the subject and topic specified in the system prompt. When a user creates a companion for "Physics - Quantum Mechanics," those exact terms are injected into the prompt, which primes GPT-4 to focus its responses on that domain.

We don't have RAG implemented yet, but it's definitely something I'd want to add for production. We could integrate a vector database like Pinecone or use Supabase's pgvector extension. For each subject, we'd store embeddings of curriculum content, textbooks, or other verified educational materials.

During a tutoring session, we'd retrieve relevant chunks based on the current conversation context and inject them into the prompt as reference material. This would make responses more accurate and curriculum-aligned.

The challenge is curating and maintaining these knowledge bases per subject, but the benefit is significantly reduced hallucinations and more trustworthy educational content, which is crucial for a tutoring platform.

---

## ðŸ”· **9. Interruption Handling (Key Vapi Feature)**

### Q30: What event from Vapi tells you the user interrupted the AI?

**Answer:**
Vapi doesn't emit a specific "interruption" event per se. Instead, the interruption is implicitly handled through the state changes we observe. When the user starts speaking while the AI is talking, Vapi detects this through Deepgram's voice activity detection.

The AI's speech is cut off, and we'd see the `speech-end` event fire for the assistant, followed quickly by new transcript messages from the user. So we infer an interruption occurred when there's a transition from assistant speaking to user speaking without a clean completion.

Vapi's automatic interruption handling ensures that the AI stops talking and the pipeline switches to listening mode immediately. From our application's perspective, we just handle the `speech-start` and `speech-end` events to update the UI - specifically stopping the Lottie animation when the assistant stops speaking.

The key is that Vapi manages the complexity of voice activity detection, audio buffer management, and context switching, so we don't need explicit interruption event handling.

---

### Q31: How do you stop ElevenLabs audio immediately on interrupt?

**Answer:**
We don't handle this directly - Vapi does it automatically. When Vapi detects that the user has started speaking through Deepgram's voice activity detection, it immediately stops sending audio to the browser and flushes the ElevenLabs audio buffer.

This is one of the major benefits of using Vapi as an orchestration layer. Managing audio interruptions at the protocol level is complex - you have to deal with WebRTC streams, buffer management, and synchronization between multiple services.

On our end, we just observe the state change through the `speech-end` event, which tells us the assistant has stopped speaking. We use this to update our UI - stopping the Lottie animation and preparing to show the user's new transcript.

If we were building this without Vapi, we'd need to implement our own audio streaming with carefully managed buffers and voice activity detection thresholds to achieve clean interruptions. Vapi abstracts all that complexity away.

---

### Q32: How does the conversation state recover after an interruption?

**Answer:**
The conversation state is maintained by GPT-4's message history, which Vapi manages throughout the session. When an interruption occurs and the user provides new input, that input is transcribed by Deepgram and appended to the conversation history as a new user message.

GPT-4 sees the full context: the previous exchange, the interrupted assistant message (even if only partially delivered), and the new user input. It can then generate an appropriate response that acknowledges the interruption or pivots to address the user's new question or comment.

From our application's perspective, we don't do anything special for recovery. We just continue listening to events and updating our transcript display. The messages array in our React state accumulates all final transcripts in order, so the UI reflects the full conversation flow including interruptions.

This seamless recovery is possible because Vapi maintains the canonical conversation state server-side, and we're just rendering a view of that state on the client.

---

### Q33: What happens if two interruptions come back-to-back?

**Answer:**
Vapi's voice activity detection would handle each interruption sequentially. If the user interrupts, starts speaking, then interrupts themselves mid-sentence to say something else, Vapi would process each speech segment as it's completed.

The first partial user input might generate a partial transcript from Deepgram. If the user stops and starts again quickly, Deepgram might send multiple partial transcripts followed by a final transcript once the user's speech stabilizes.

GPT-4 would only generate a response once it receives a complete user message. If the user is rapidly interrupting themselves, the assistant would essentially wait until there's a clear, final transcript to respond to.

From our UI perspective, we'd see the `isSpeaking` state flip back and forth rapidly, potentially causing the Lottie animation to start and stop. In practice, Vapi has built-in debouncing and voice activity thresholds to prevent this kind of jitter, so extremely rapid interruptions are smoothed out into coherent speech segments.

---

## ðŸ”· **10. Storage & Database**

### Q34: What data model did you use for companions, sessions, transcripts, subscriptions?

**Answer:**
We use Supabase with a relational model. The `companions` table has columns for id, name, subject, topic, duration, voice, style, author (foreign key to Clerk user ID), and timestamps.

The `session_history` table tracks interactions with columns for id, companion_id (foreign key to companions), user_id (references Clerk user), and created_at timestamp. This is a simple join table that creates a many-to-many relationship between users and companions, recording each tutoring session.

We don't currently have a dedicated `transcripts` table, but if we implemented it, it would have session_id, message_role (user or assistant), content, and timestamp columns.

For subscriptions, we rely on Clerk's built-in user metadata and session claims rather than storing subscription data in our database. Clerk manages the subscription state, and we query it using their `has()` authorization method.

This model is simple but effective for our current needs. The companion and session history tables give us the core data we need for the tutoring platform, and Clerk handles the complexity of subscription management.

---

### Q35: Why did you use Supabase/Convex/Postgres?

**Answer:**
We chose Supabase because it provides a Postgres database with a great developer experience. It has a generous free tier, built-in authentication that integrates well with Clerk, and row-level security features.

The Supabase JavaScript client makes it easy to query data with a chainable API that feels natural in a TypeScript application. We can do complex queries with filters, joins, and ordering without writing raw SQL.

Supabase also offers real-time subscriptions if we wanted to add features like live updates when new companions are created. The built-in storage for file uploads is another benefit we could use for companion avatars or user profiles.

Compared to managing our own Postgres instance or using a traditional ORM, Supabase gives us database management, automatic backups, and a nice dashboard for inspecting data, all with minimal setup. It's a great fit for a Next.js application because both are server-side-friendly with good TypeScript support.

---

### Q36: How do you query the last 10 sessions efficiently?

**Answer:**
We use the `recentSession` server action which queries Supabase with `.order('created_at', { ascending: false }).limit(10)`. This fetches the 10 most recent session records sorted by timestamp.

We also use a select statement with a join: `.select('companions:companion_id(*)')` which pulls in the full companion data for each session in a single query. This is more efficient than fetching sessions first and then making separate queries for each companion.

Supabase handles the SQL join behind the scenes, and we get back an array where each session object contains the full companion details. We then map over this data to extract just the companion objects for display.

To make this even more efficient at scale, we'd add a database index on the `created_at` column in the session_history table and on the `companion_id` foreign key. With these indexes, the order and join operations would be very fast even with thousands of sessions.

---

## ðŸ”· **11. Scalability & Performance**

### Q37: How would your app behave with 10,000 users actively calling at once?

**Answer:**
That's a great stress test question. Our architecture would face several bottlenecks. First, Vapi itself has concurrent call limits based on our subscription tier. We'd need to ensure we're on a plan that supports that volume.

Second, our Supabase free tier has connection limits and read/write quotas. With 10,000 concurrent users, we'd likely exceed the connection pool and need to upgrade to a paid plan with higher limits and potentially implement connection pooling.

Third, Next.js server actions would be handling numerous requests for session creation and updates. We'd need to deploy on infrastructure that can auto-scale - Vercel handles this well, but we'd want to ensure our serverless function concurrency limits are sufficient.

Fourth, Clerk's authentication would need to handle 10,000 simultaneous token verifications. Clerk is designed for scale, but we'd want to monitor rate limits.

For this scale, we'd also want to implement caching for frequently accessed data like companion details, use a CDN for static assets, and potentially move to a dedicated backend API rather than server actions for better control over connection pooling and resource management.

---

### Q38: What are the scaling limitations of Vapi?

**Answer:**
Vapi's main scaling limitations come from concurrent call limits based on your pricing tier. The free or starter tiers might support only a handful of concurrent calls, while enterprise plans support more.

Cost is another factor - each call consumes credits based on duration, and at scale, the costs from Deepgram, GPT-4, and ElevenLabs add up quickly. Managing per-user budgets or implementing call duration limits would be important for cost control.

Latency can also be impacted by geographic distribution. Vapi's servers are in specific regions, so users far from those regions might experience higher latency. For global scale, we'd want to work with Vapi to ensure optimal routing or potentially use edge functions.

Finally, Vapi is a managed service, so we're dependent on their infrastructure reliability. If Vapi experiences downtime, our entire voice tutoring feature is unavailable. For mission-critical applications, we'd want SLAs and redundancy plans, or even consider building our own orchestration layer for full control, though that's significantly more complex.

---

### Q39: Would your architecture change if you had to introduce group voice tutoring?

**Answer:**
Absolutely, group tutoring would require significant architectural changes. First, we'd need multi-party audio mixing. Vapi currently handles one-to-one conversations, so we'd need to explore if they support conference-style calls or integrate a different solution like Twilio Programmable Voice or Agora.

We'd need a different GPT-4 prompt structure that handles multiple speakers. The model would need to track which student asked which question and address them by name. This might require a more sophisticated context management system.

The UI would need to show all participants, their speaking status, and potentially individual mute controls. We'd likely move from a simple React state model to a more robust state management solution like Zustand or Redux to handle multiple user states.

Database schema would expand to include a `group_sessions` table with participant tracking. We'd need permissions logic for who can join which group, potentially instructor-led sessions versus peer study groups.

Real-time collaboration features like shared whiteboards or collaborative notes would also be expected in group tutoring, requiring WebSocket connections or a real-time database like Supabase's real-time features. It's a much more complex product than one-on-one tutoring.

---

## ðŸ”· **12. Testing, Monitoring & Logging**

### Q40: How did you test voice call flows locally?

**Answer:**
Testing voice calls locally is straightforward because Vapi's SDK works in development mode. We run `npm run dev` and navigate to a companion session page in the browser.

The actual voice pipeline uses production services - Deepgram, GPT-4, and ElevenLabs - even in local development because Vapi orchestrates them through their cloud infrastructure. We just need our API keys configured in the environment variables.

For testing, we manually initiate calls, speak different types of queries, test interruptions by speaking while the AI is responding, and verify that transcripts appear correctly in the UI. We also test edge cases like muting and unmuting, ending calls prematurely, and checking that session history is recorded.

For more systematic testing, we could write integration tests using Playwright or Cypress to automate the browser interactions, though testing actual voice input/output would be challenging. We could mock the Vapi SDK responses to test UI state transitions without making real API calls, which would be important for CI/CD pipelines.

---

### Q41: How do you log real-time events from Vapi without slowing down the call?

**Answer:**
Currently, we use `console.log` for error events from Vapi, which isn't ideal for production. For real-time event logging without impacting performance, we'd want to send logs asynchronously to a logging service.

We could integrate something like Sentry - which we already have configured in the project - to capture errors and exceptions. We'd wrap Vapi event handlers in try-catch blocks and report errors to Sentry with context about the call state and user.

For general event tracking, we could use a service like LogRocket or Datadog. When events like `call-start`, `message`, or `speech-start` fire, we'd send a lightweight event payload to the logging service asynchronously without blocking the main thread.

The key is to avoid heavy processing in the event handlers themselves. We'd queue log events in memory and batch-send them, or use fire-and-forget HTTP requests that don't wait for responses. This ensures the real-time call flow isn't impacted by logging overhead.

We'd also implement sampling for high-frequency events like partial transcripts to avoid log flooding while still capturing enough data for debugging.

---

### Q42: How do you monitor errors (e.g., audio failures)?

**Answer:**
Right now, we have basic error monitoring through the Vapi error event handler where we log errors to the console. With Sentry already integrated in our project via the config files, we'd want to properly utilize it.

When a Vapi error occurs, we'd capture it using `Sentry.captureException()` along with contextual data like the companion ID, user ID, and call status. This would give us centralized error tracking with stack traces and user session data.

For audio-specific failures, we'd add custom error boundaries in our React components. If the CompanionComponent crashes due to audio issues, the error boundary would catch it, log to Sentry, and show a user-friendly error message.

We'd also implement health check monitoring. If Vapi, Supabase, or Clerk services are unreachable, we'd want alerts. This could be done with uptime monitoring tools like Better Uptime or custom health check endpoints that we ping regularly.

For production, we'd set up dashboards in Sentry to track error rates, common failure patterns, and user-impacting issues, with alerts configured for critical errors that need immediate attention.

---

### Q43: What metrics would you collect for production debugging?

**Answer:**
I'd track several categories of metrics. For call quality: average call duration, successful call completion rate, interruption frequency, time-to-first-response (latency from user speech to AI response start).

For user behavior: companions created per user, subjects most frequently accessed, peak usage times, session frequency per user. This helps identify popular features and usage patterns.

For errors: failed call initiation rate, audio stream errors, authentication failures, database query errors. We'd break these down by error type and severity.

For performance: API response times for companion fetching and creation, database query durations, Supabase connection pool utilization.

For business metrics: conversion rate from free to paid tiers, companion creation limits hit (indicating upgrade intent), session length by subject.

We'd use a combination of tools: Sentry for errors, Vercel Analytics for web vitals and page performance, Supabase's built-in query performance dashboard, and potentially a custom analytics solution or Mixpanel for user behavior tracking. The goal is comprehensive observability without overwhelming ourselves with data.

---

## ðŸ”· **13. Edge Cases**

### Q44: What happens if the user does not speak for 10 seconds?

**Answer:**
Vapi has configurable timeout settings, but by default, if the user is silent for an extended period during their turn to speak, Vapi might prompt them or the assistant might say something like "Are you still there?"

In our current implementation, we don't have explicit timeout handling in the UI. The call would remain active, and the assistant would wait for user input. We could improve this by adding a timeout warning in the UI - maybe showing a message after 15 seconds of silence asking if they want to continue.

For better UX, we could also configure the GPT-4 assistant to proactively engage after silence, asking questions like "Do you have any questions so far?" or "Should we move on to the next topic?"

We'd want to avoid automatically ending the call due to silence because the user might be thinking or taking notes. Instead, gentle prompts from the AI or UI notifications would keep the session alive while re-engaging the user.

---

### Q45: What if Deepgram sends empty text for a whisper or noise?

**Answer:**
Deepgram's voice activity detection is quite good at filtering out non-speech sounds, but it's not perfect. If Deepgram sends an empty string or very short transcripts for whispers or background noise, we have some safeguards.

In our message handling, we only process transcript messages with type "final", which means Deepgram is confident it's actual speech. Partial transcripts that are just noise would typically not be finalized.

However, if an empty final transcript did come through, our code would still add it to the messages array. To prevent this, we should add a filter: only append messages where `message.transcript.trim().length > 0`. This would filter out empty or whitespace-only transcripts.

For whispers that produce incomplete transcripts, GPT-4 would receive them as context but would likely respond asking for clarification or repeating the question, which is actually good tutoring behavior. The key is ensuring our UI doesn't display empty transcript bubbles, which would be confusing for users.

---

### Q46: What can cause ElevenLabs to return partial audio?

**Answer:**
ElevenLabs streams audio in chunks as it generates speech. In normal operation, you'd receive all chunks sequentially until the full utterance is complete. Partial audio delivery is actually expected and handled by Vapi's buffering.

However, issues can arise from network interruptions between Vapi and ElevenLabs, or if ElevenLabs' service experiences issues mid-generation. In these cases, the audio might cut off unexpectedly.

API rate limiting could also cause partial responses if we exceed ElevenLabs' quota. The service might return only partial audio before hitting the limit.

From our application's perspective, we wouldn't directly detect partial audio because Vapi handles the audio streaming. We'd observe the issue through user reports of choppy or incomplete AI responses. The `speech-end` event would still fire even if audio was partial.

To mitigate, we'd monitor error events from Vapi, implement retry logic for failed audio generation, and ensure our ElevenLabs subscription tier supports our expected call volume. Logging audio generation latency and completion rates would help identify when this is occurring.

---

### Q47: How do you detect if the user unexpectedly hangs up the call?

**Answer:**
Vapi's `call-end` event fires regardless of how the call ends - whether the user clicks our disconnect button, closes the browser tab, or loses network connection.

In our `useEffect` hook, we have a listener for `call-end` that sets the call status to FINISHED and triggers `addToSessionHistory` to record the session in the database. This happens for both graceful exits and unexpected disconnections.

We don't currently distinguish between user-initiated endings and errors. To improve this, we could check if the call-end event includes error information or check the call duration. If it's very short, it might indicate a connection failure rather than intentional hangup.

For unexpected disconnections due to network issues, we could add logic to show a reconnection prompt if the call ends within a few seconds of starting, offering to retry. For normal hangups, we'd show a session summary or feedback form.

Browser events like `beforeunload` could also help detect when users are closing the tab, allowing us to gracefully end the Vapi call and save state before the page unloads.

---

## ðŸ”· **14. Deployment & DevOps**

### Q48: How do you deploy your Next.js app? Vercel? Custom?

**Answer:**
Given that this is a Next.js 15 app, Vercel would be the natural deployment choice because of its seamless integration. You just connect your GitHub repository, and Vercel automatically builds and deploys on every push to main.

Vercel handles serverless function deployment for our API routes and server actions, manages environment variables securely through their dashboard, and provides automatic HTTPS and CDN distribution globally.

The build process runs `npm run build` which creates optimized production bundles. Server components and client components are properly separated, and Vercel's edge network ensures fast page loads worldwide.

For environment variables, we'd configure them in Vercel's dashboard: `NEXT_PUBLIC_VAPI_API_KEY`, `NEXT_PUBLIC_SUPABASE_URL`, `NEXT_PUBLIC_SUPABASE_ANON_KEY`, and Clerk keys. Vercel encrypts these and injects them at runtime.

If we needed more control or had specific infrastructure requirements, we could deploy to a custom setup using Docker containers on AWS ECS or Google Cloud Run, but Vercel's simplicity and performance optimizations for Next.js make it the ideal choice for most use cases.

---

### Q49: How do you handle environment variables safely?

**Answer:**
We follow Next.js best practices for environment variables. Variables prefixed with `NEXT_PUBLIC_` are embedded in the browser bundle and are safe for client-side code, like the Vapi public API key and Supabase URL.

Sensitive secrets like Supabase's service role key or Clerk's webhook signing secret would be stored without the `NEXT_PUBLIC_` prefix, making them available only in server-side code and API routes. They never get sent to the browser.

Locally, we use a `.env.local` file that's git-ignored. In production on Vercel, we set these through the dashboard. Vercel encrypts environment variables and provides different values for preview and production deployments.

For extra security, we'd use Vercel's environment variable encryption features and potentially integrate with a secrets manager like AWS Secrets Manager or HashiCorp Vault for highly sensitive credentials.

We also ensure that server actions and API routes never accidentally expose environment variables in error messages or responses. Any errors are logged server-side only, and sanitized error messages are returned to clients.

---

### Q50: How do you roll back deployments without losing active sessions?

**Answer:**
Vercel makes rollbacks straightforward. Each deployment is immutable and versioned, so you can instantly revert to a previous deployment through the Vercel dashboard or CLI with `vercel rollback`.

However, active voice calls are a concern. Since Vapi maintains the call state and our frontend just connects to it via WebSocket, rolling back the deployment would disconnect active clients when their browser updates or they refresh.

To handle this gracefully, we'd want to implement a few strategies. First, Vercel's gradual rollouts can help - deploy to a percentage of traffic first and monitor for issues before full rollout.

Second, we could maintain WebSocket connections across deployments by ensuring the Vapi client state is resilient. Since Vapi is external, the actual call might continue even if our page reloads, though the UI would disconnect.

For truly zero-downtime rollbacks with active sessions, we'd need stateful connection handling, possibly using a separate WebSocket server that persists across deployments, or ensuring that Vapi reconnection logic is robust enough that users can seamlessly rejoin their session after a page refresh.

In practice, we'd schedule deployments during low-traffic periods and communicate maintenance windows to users for updates that might affect active calls.

---

## âœ… **Conclusion**

These answers reflect the actual implementation in your Dumbledore AI project, focusing on the technical details of Next.js 15, Vapi integration, Supabase database, Clerk authentication, and real-time voice AI tutoring. Each answer is crafted to be concise yet comprehensive, suitable for a technical interview at Tech Mahindra or similar companies.

**Key Strengths Demonstrated:**
- Full-stack Next.js expertise with server/client component patterns
- Real-time voice AI integration with multiple third-party services
- Secure authentication and authorization with tier-based feature gating
- Database design and efficient querying with Supabase
- Event-driven architecture for real-time UI synchronization
- Production-ready considerations for scaling, monitoring, and deployment

Good luck with your interviews! ðŸš€